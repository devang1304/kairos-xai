{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the sample size to test the preprocess file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit_hyperlinks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 93)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200 = df[:200]\n",
    "df_200.to_csv(\"reddit_hyperlinks_200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>i</th>\n",
       "      <th>u</th>\n",
       "      <th>ts</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>...</th>\n",
       "      <th>f80</th>\n",
       "      <th>f81</th>\n",
       "      <th>f82</th>\n",
       "      <th>f83</th>\n",
       "      <th>f84</th>\n",
       "      <th>f85</th>\n",
       "      <th>f86</th>\n",
       "      <th>f87</th>\n",
       "      <th>f88</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>1388507998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.069470</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.754070</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5599</td>\n",
       "      <td>1</td>\n",
       "      <td>1388513917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.739638</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4502</td>\n",
       "      <td>2</td>\n",
       "      <td>1388588075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.750365</td>\n",
       "      <td>0.027626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>1388511475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.026099</td>\n",
       "      <td>0.022889</td>\n",
       "      <td>0.770335</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3835</td>\n",
       "      <td>4</td>\n",
       "      <td>1388544673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>0.014573</td>\n",
       "      <td>0.775902</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     i  u          ts   f1        f2        f3        f4  \\\n",
       "0           0    55  0  1388507998  1.0  0.069470  0.007186  0.006333   \n",
       "1           1  5599  1  1388513917  0.0  0.000000  0.001262  0.001246   \n",
       "2           2  4502  2  1388588075  1.0  0.000000  0.000874  0.000916   \n",
       "3           3    44  3  1388511475  1.0  0.023766  0.026099  0.022889   \n",
       "4           4  3835  4  1388544673  1.0  0.020110  0.016169  0.014573   \n",
       "\n",
       "         f5        f6  ...       f80  f81  f82  f83  f84  f85  f86       f87  \\\n",
       "0  0.754070  0.020411  ...  0.110405  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "1  0.739638  0.023245  ...  0.213450  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "2  0.750365  0.027626  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "3  0.770335  0.002060  ...  0.066703  0.0  0.0  0.0  0.0  0.0  0.0  0.203125   \n",
       "4  0.775902  0.008189  ...  0.080044  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "\n",
       "        f88  label  \n",
       "0  0.000000      0  \n",
       "1  0.000000      0  \n",
       "2  0.000000      0  \n",
       "3  0.017857      0  \n",
       "4  0.028571      0  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFile = df.to_csv(\"reddit_hyperlinks_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import node\n",
    "from matplotlib import use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_first_two_directories(path):\n",
    "    path_parts = path.split(os.sep)[3:]  # Split the path and discard the first 3 parts\n",
    "    new_path = os.sep.join(path_parts)  # Rejoin the remaining parts\n",
    "    return new_path\n",
    "\n",
    "# Get the current working directory\n",
    "full_dir = os.getcwd()\n",
    "modified_dir = remove_first_two_directories(full_dir)\n",
    "pruned_dir=Path(modified_dir).parents[4]\n",
    "p=str(\"/home/\"+str(pruned_dir))\n",
    "sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dataset_train_flag(df):\n",
    "    labels = df['label'].to_numpy()\n",
    "    mask = (labels == 1) | (labels == 0)\n",
    "    return mask\n",
    "\n",
    "def rename_columns_wiki_reddit(file):\n",
    "    \n",
    "    df = pd.read_csv(file, skiprows=1, header=None)\n",
    "    feat_nums = df.shape[1] - 4\n",
    "    new_columns = ['u', 'i', 'ts', 'label']\n",
    "    \n",
    "    for i in range(feat_nums):\n",
    "        new_columns.append( f'f{i}' )\n",
    "    \n",
    "    rename_dict = {i: new_columns[i] for i in range(len(new_columns))}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    df.to_csv(file, index=False)\n",
    "    print(f'rename the columns of {file}.')\n",
    "\n",
    "def reindex(df):\n",
    "    df['i'] += df['u'].max() + 1\n",
    "    df['u'] += 1\n",
    "    df['i'] += 1\n",
    "    df['e_idx'] = df.index.values + 1\n",
    "    df['idx'] = df.e_idx\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions imported from the original code\n",
    "#Source: from tgnnexplainer.xgraph.dataset.tg_dataset import verify_dataframe_unify, check_wiki_reddit_dataformat\n",
    "\n",
    "def verify_dataframe_unify(df):\n",
    "    for col in ['u', 'i', 'ts', 'label', 'e_idx', 'idx']:\n",
    "        assert col in df.columns.to_list()\n",
    "    \n",
    "#    assert df.iloc[:, 0].min() == 1\n",
    "#    assert df.iloc[:, 0].max() == df.iloc[:, 0].nunique()\n",
    "#    assert df.iloc[:, 1].min() == df.iloc[:, 0].max() + 1\n",
    "#    assert df.iloc[:, 1].max() == df.iloc[:, 0].max() + df.iloc[:, 1].nunique()\n",
    "    assert df['e_idx'].min() == 1\n",
    "    assert df['e_idx'].max() == len(df)\n",
    "    assert df['idx'].min() == 1\n",
    "    assert df['idx'].max() == len(df)\n",
    "    \n",
    "def check_wiki_reddit_dataformat(df):\n",
    "    assert df.iloc[:, 0].min() == 0\n",
    "#    assert df.iloc[:, 0].max() + 1 == df.iloc[:, 0].nunique() # 0, 1, 2, ...\n",
    "    assert df.iloc[:, 1].min() == 0\n",
    " #   assert df.iloc[:, 1].max() + 1 == df.iloc[:, 1].nunique() # 0, 1, 2, ...\n",
    "    \n",
    "    for col in ['u', 'i', 'ts', 'label']:\n",
    "        assert col in df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_name, out_dir=None):\n",
    "    # from tgnnexplainer.__init__ import ROOT_DIR\n",
    "    # from tgnnexplainer.xgraph.dataset.tg_dataset import verify_dataframe_unify, check_wiki_reddit_dataformat\n",
    "\n",
    "    data_path = '/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 789 Explainable AI/XAI-Cybersec/XAI-CyberSec/tgnnexplainer/reddit_hyperlinks_10k.csv'\n",
    "\n",
    "    # PATH = './processed/{}.csv'.format(data_name)\n",
    "    # if out_dir is None:\n",
    "    #     out_dir = Path('./processed/')\n",
    "    # else:\n",
    "    #     out_dir = Path(out_dir)\n",
    "\n",
    "    OUT_DF = 'ml_{}.csv'.format(data_name)\n",
    "    OUT_EDGE_FEAT = 'ml_{}.npy'.format(data_name)\n",
    "    OUT_NODE_FEAT = 'ml_{}_node.npy'.format(data_name)\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # the very raw format\n",
    "    if 'comma_separated_list_of_features' in df.columns.tolist():\n",
    "        rename_columns_wiki_reddit(data_path)\n",
    "        df = pd.read_csv(data_path)\n",
    "\n",
    "    check_wiki_reddit_dataformat(df)\n",
    "\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    df = reindex(df)\n",
    "    verify_dataframe_unify(df)\n",
    "    \n",
    "    new_df = df\n",
    "\n",
    "    # set edge feature and node feature\n",
    "    if data_name == 'simulate_v2':\n",
    "        raise NotImplementedError\n",
    "    elif data_name == 'simulate_v1':\n",
    "        raise NotImplementedError\n",
    "\n",
    "    elif data_name == 'wikipedia' or data_name == 'reddit' or data_name == 'mooc' or data_name == 'reddit_v1' or data_name == 'reddit_hyperlinks': # added mooc dataset\n",
    "        select_columns = [c for c in new_df.columns if 'f' in c] # features\n",
    "        edge_feat = np.zeros((len(df) + 1, len(select_columns))) # 0-th pad with 0\n",
    "        edge_feat[1:, :] = new_df[select_columns].to_numpy()\n",
    "\n",
    "        edge_feat_dim = edge_feat.shape[1]\n",
    "        num_nodes = new_df.i.max()\n",
    "        node_feat = np.zeros((num_nodes + 1, edge_feat_dim))\n",
    "\n",
    "    else: \n",
    "        raise NotImplementedError\n",
    "\n",
    "    assert len(node_feat) == new_df.i.max() + 1\n",
    "    assert len(edge_feat) == len(new_df) + 1\n",
    "\n",
    "    print('dataset: ', data_name)\n",
    "    print('edge feature shape: ', edge_feat.shape)\n",
    "    print('node feature shape: ', node_feat.shape)\n",
    "    new_df[['u', 'i', 'ts', 'label', 'idx', 'e_idx']].to_csv(OUT_DF, index=False)\n",
    "    np.save(OUT_EDGE_FEAT, edge_feat) # edge feature matrix\n",
    "    np.save(OUT_NODE_FEAT, node_feat) # node feature matrix\n",
    "    print(f'{OUT_DF} saved')\n",
    "    print(f'{OUT_EDGE_FEAT} saved')\n",
    "    print(f'{OUT_NODE_FEAT} saved')\n",
    "\n",
    "\n",
    "def process_garden_5():\n",
    "    from tgnnexplainer import ROOT_DIR\n",
    "    data_dir = ROOT_DIR/'xgraph'/'dataset'/'data'\n",
    "    data_path = data_dir/'garden_5.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    if 'label' not in df.columns.to_list():\n",
    "        df['label'] = np.ones((len(df),))\n",
    "        df.to_csv(data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  reddit\n",
      "edge feature shape:  (10001, 88)\n",
      "node feature shape:  (23622, 88)\n",
      "ml_reddit.csv saved\n",
      "ml_reddit.npy saved\n",
      "ml_reddit_node.npy saved\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('-d', '--data', type=str, default='simulate')\n",
    "    # parser.add_argument('-rename_w_r', action='store_true', help='rename columns of wikipedia and reddit')\n",
    "    # args = parser.parse_args()\n",
    "    # dataset = args.data\n",
    "\n",
    "    run(\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
